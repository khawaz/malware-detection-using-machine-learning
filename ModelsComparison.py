import pickle
import numpy
import pandas
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score, recall_score, f1_score
from sklearn import preprocessing
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

LRmodel = pickle.load(open("models/LogisticRegressionModel.pkl", 'rb'))
KNNmodel = pickle.load(open("models/KNNModel.pkl", 'rb'))
NBmodel = pickle.load(open("models/NaiveBayesModel.pkl", 'rb'))
DTmodel = pickle.load(open("models/DecisionTreesModel.pkl", 'rb'))
RFmodel = pickle.load(open("models/RandomForestModel.pkl", 'rb'))
MLPmodel = pickle.load(open("models/NNModel.pkl", 'rb'))

file = pandas.read_csv("TestingData.csv")


features = file.drop("legitimate", axis=1)
labels = file["legitimate"]

features = numpy.array(features).reshape(-1, 8)
labels = numpy.array(labels)

processed_features = preprocessing.MinMaxScaler().fit_transform(features)

LR_proba = LRmodel.predict_proba(processed_features)
LR_proba = LR_proba[:, 1]
LR_predictions = LRmodel.predict(processed_features)

KNN_proba = KNNmodel.predict_proba(processed_features)
KNN_proba = KNN_proba[:, 1]
KNN_predictions = KNNmodel.predict(processed_features)

NB_proba = NBmodel.predict_proba(processed_features)
NB_proba = NB_proba[:, 1]
NB_predictions = NBmodel.predict(processed_features)

DT_proba = DTmodel.predict_proba(features)
DT_proba = DT_proba[:, 1]
DT_predictions = DTmodel.predict(features)

RF_proba = RFmodel.predict_proba(features)
RF_proba = RF_proba[:, 1]
RF_predictions = RFmodel.predict(features)

MLP_proba = MLPmodel.predict_proba(processed_features)
MLP_proba = MLP_proba[:, 1]
MLP_predictions = MLPmodel.predict(processed_features)

LR_fpr, LR_tpr , LR_threshold = roc_curve(labels,LR_proba)
LR_aucScore = auc(LR_fpr,LR_tpr)

KNN_fpr, KNN_tpr , KNN_threshold = roc_curve(labels,KNN_proba)
KNN_aucScore = auc(KNN_fpr,KNN_tpr)

NB_fpr, NB_tpr , NB_threshold = roc_curve(labels,NB_proba)
NB_aucScore = auc(NB_fpr,NB_tpr)

DT_fpr, DT_tpr , DT_threshold = roc_curve(labels,DT_proba)
DT_aucScore = auc(DT_fpr,DT_tpr)

RF_fpr, RF_tpr , RF_threshold = roc_curve(labels,RF_proba)
RF_aucScore = auc(RF_fpr,RF_tpr)

MLP_fpr, MLP_tpr , MLP_threshold = roc_curve(labels,MLP_proba)
MLP_aucScore = auc(MLP_fpr,MLP_tpr)

print("\n\n\n")

print("Logistic Regression Scores :- ")
print("Confusion Matrix :- \n", confusion_matrix(labels,LR_predictions))
print("Accuracy :- ", accuracy_score(labels,LR_predictions))
print("Precision :- ", precision_score(labels,LR_predictions))
print("Recall :- ", recall_score(labels,LR_predictions))
print("F-1 :- ", f1_score(labels,LR_predictions))


print("\n\n\n")

print("KNN Scores :- ")
print("Confusion Matrix :- \n", confusion_matrix(labels,KNN_predictions))
print("Accuracy :- ", accuracy_score(labels,KNN_predictions))
print("Precision :- ", precision_score(labels,KNN_predictions))
print("Recall :- ", recall_score(labels,KNN_predictions))
print("F-1 :- ", f1_score(labels,KNN_predictions))


print("\n\n\n")

print("Naive Bayes Scores :- ")
print("Confusion Matrix :- \n", confusion_matrix(labels,NB_predictions))
print("Accuracy :- ", accuracy_score(labels,NB_predictions))
print("Precision :- ", precision_score(labels,NB_predictions))
print("Recall :- ", recall_score(labels,NB_predictions))
print("F-1 :- ", f1_score(labels,NB_predictions))

print("\n\n\n")

print("Decision Trees Scores :- ")
print("Confusion Matrix :- \n", confusion_matrix(labels,DT_predictions))
print("Accuracy :- ", accuracy_score(labels,DT_predictions))
print("Precision :- ", precision_score(labels,DT_predictions))
print("Recall :- ", recall_score(labels,DT_predictions))
print("F-1 :- ", f1_score(labels,DT_predictions))

print("\n\n\n")

print("Random Forest Scores :- ")
print("Confusion Matrix :- \n", confusion_matrix(labels,RF_predictions))
print("Accuracy :- ", accuracy_score(labels,RF_predictions))
print("Precision :- ", precision_score(labels,RF_predictions))
print("Recall :- ", recall_score(labels,RF_predictions))
print("F-1 :- ", f1_score(labels,RF_predictions))

print("\n\n\n")

print("MLP Scores :- ")
print("Confusion Matrix :- \n", confusion_matrix(labels,MLP_predictions))
print("Accuracy :- ", accuracy_score(labels,MLP_predictions))
print("Precision :- ", precision_score(labels,MLP_predictions))
print("Recall :- ", recall_score(labels,MLP_predictions))
print("F-1 :- ", f1_score(labels,MLP_predictions))



plt.figure(figsize=(5,5), dpi=100)
plt.plot(LR_fpr,LR_tpr,linestyle = "-",color = "b",label = "Logistic Regression (auc = %0.3f)" % LR_aucScore)
plt.plot(KNN_fpr,KNN_tpr,linestyle = "-",color = "g",label = "KNN (auc = %0.3f)" % KNN_aucScore)
plt.plot(NB_fpr,NB_tpr,linestyle = "-",color = "r",label = "Naive Bayes (auc = %0.3f)" % NB_aucScore)
plt.plot(DT_fpr,DT_tpr,linestyle = "-",color = "c",label = "Decision Trees (auc = %0.3f)" % DT_aucScore)
plt.plot(RF_fpr,RF_tpr,linestyle = "-",color = "m",label = "Random Forests (auc = %0.3f)" % RF_aucScore)
plt.plot(MLP_fpr,MLP_tpr,linestyle = "-",color = "y",label = "MLP (auc = %0.3f)" % MLP_aucScore)
plt.xlabel('False Positive Rate -->')
plt.ylabel("True Positive Rate -->")

plt.legend()
plt.show()

